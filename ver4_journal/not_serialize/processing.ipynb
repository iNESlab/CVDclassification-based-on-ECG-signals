{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:35:04.720039Z",
     "start_time": "2024-10-09T00:35:04.716704Z"
    }
   },
   "id": "4a2ddd3ff0184e55",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 현재 작업 디렉토리 가져오기\n",
    "current_dir = os.getcwd()  # 현재 Jupyter Notebook의 작업 디렉토\n",
    "env_path = os.path.join(current_dir, '../.env')\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv(env_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:36:25.651683Z",
     "start_time": "2024-10-09T00:36:25.642589Z"
    }
   },
   "id": "8bd25b100b584a0c",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   ecg_id  patient_id   age  sex  height  weight  nurse  site     device  \\\n0       1     15709.0  56.0    1     NaN    63.0    2.0   0.0  CS-12   E   \n1       2     13243.0  19.0    0     NaN    70.0    2.0   0.0  CS-12   E   \n2       3     20372.0  37.0    1     NaN    69.0    2.0   0.0  CS-12   E   \n3       4     17014.0  24.0    0     NaN    82.0    2.0   0.0  CS-12   E   \n4       5     17448.0  19.0    1     NaN    70.0    2.0   0.0  CS-12   E   \n\n        recording_date  ... validated_by_human  baseline_drift static_noise  \\\n0  1984-11-09 09:17:34  ...               True             NaN    , I-V1,     \n1  1984-11-14 12:55:37  ...               True             NaN          NaN   \n2  1984-11-15 12:49:10  ...               True             NaN          NaN   \n3  1984-11-15 13:44:57  ...               True    , II,III,AVF          NaN   \n4  1984-11-17 10:43:15  ...               True   , III,AVR,AVF          NaN   \n\n  burst_noise electrodes_problems  extra_beats  pacemaker  strat_fold  \\\n0         NaN                 NaN          NaN        NaN           3   \n1         NaN                 NaN          NaN        NaN           2   \n2         NaN                 NaN          NaN        NaN           5   \n3         NaN                 NaN          NaN        NaN           3   \n4         NaN                 NaN          NaN        NaN           4   \n\n                 filename_lr                filename_hr  \n0  records100/00000/00001_lr  records500/00000/00001_hr  \n1  records100/00000/00002_lr  records500/00000/00002_hr  \n2  records100/00000/00003_lr  records500/00000/00003_hr  \n3  records100/00000/00004_lr  records500/00000/00004_hr  \n4  records100/00000/00005_lr  records500/00000/00005_hr  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ecg_id</th>\n      <th>patient_id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>nurse</th>\n      <th>site</th>\n      <th>device</th>\n      <th>recording_date</th>\n      <th>...</th>\n      <th>validated_by_human</th>\n      <th>baseline_drift</th>\n      <th>static_noise</th>\n      <th>burst_noise</th>\n      <th>electrodes_problems</th>\n      <th>extra_beats</th>\n      <th>pacemaker</th>\n      <th>strat_fold</th>\n      <th>filename_lr</th>\n      <th>filename_hr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15709.0</td>\n      <td>56.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>63.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-09 09:17:34</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>, I-V1,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>records100/00000/00001_lr</td>\n      <td>records500/00000/00001_hr</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>13243.0</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-14 12:55:37</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>records100/00000/00002_lr</td>\n      <td>records500/00000/00002_hr</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20372.0</td>\n      <td>37.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>69.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-15 12:49:10</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>records100/00000/00003_lr</td>\n      <td>records500/00000/00003_hr</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>17014.0</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>82.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-15 13:44:57</td>\n      <td>...</td>\n      <td>True</td>\n      <td>, II,III,AVF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>records100/00000/00004_lr</td>\n      <td>records500/00000/00004_hr</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>17448.0</td>\n      <td>19.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-17 10:43:15</td>\n      <td>...</td>\n      <td>True</td>\n      <td>, III,AVR,AVF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>records100/00000/00005_lr</td>\n      <td>records500/00000/00005_hr</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_path=os.getenv(\"DATA_PATH\")\n",
    "ecg_data=pd.read_csv(f'{db_path}/ptbxl_database.csv')\n",
    "ecg_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:36:27.316525Z",
     "start_time": "2024-10-09T00:36:27.215522Z"
    }
   },
   "id": "41d7712d02c12fdc",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 및 하이퍼파라미터 준비\n",
    "BCResNet의 cnn_first의 stride나 다른 conv의 stride에 따라 입력 또는 출력의 크기가 달라짐.\n",
    "이는 조절해가면서 실험필요"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92dc6ab07ba70006"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:36:28.666607Z",
     "start_time": "2024-10-09T00:36:28.532261Z"
    }
   },
   "id": "a0c7f0b5f5c4c757",
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 5])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ver4_journal.common.network.BCResNets import BCResNets\n",
    "\n",
    "tau = 1\n",
    "in_channel = 12\n",
    "model = BCResNets(int(tau * 8), 5, in_channel)\n",
    "\n",
    "if in_channel == 1:\n",
    "    sample = torch.randn(1,12,1000)\n",
    "else:\n",
    "    sample = torch.randn(1,12,1000)\n",
    "    #sample = torch.randn(1,12,20,48)\n",
    "\n",
    "print(sample.shape)\n",
    "pred = model(sample)\n",
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:36:28.894713Z",
     "start_time": "2024-10-09T00:36:28.850642Z"
    }
   },
   "id": "8060e95182a8bfb7",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count in ECG_train: 9415\n",
      "Class 1 count in ECG_train: 7596\n",
      "Class weights: {0: np.float64(1.8067976633032394), 1: np.float64(2.239468141126909)}\n",
      "Weight tensor: tensor([1.8068, 2.2395])\n",
      "Batch shape: torch.Size([32, 12, 1000])\n",
      "532 67 68\n",
      "Using mps device\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 121\u001B[0m\n\u001B[1;32m    119\u001B[0m train_accuracy \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    120\u001B[0m train_loss_sum \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m--> 121\u001B[0m \u001B[43m\u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m:\u001B[49m\n\u001B[1;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzero_grad\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:673\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    671\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    672\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 673\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    674\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    675\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     50\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 52\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     53\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     54\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/ver4_journal/common/data/ECG_Data.py:50\u001B[0m, in \u001B[0;36mECG_Data.__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__getitem__\u001B[39m(\u001B[38;5;28mself\u001B[39m, idx):\n\u001B[1;32m     49\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbase_path \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mfilename_lr\u001B[39m\u001B[38;5;124m'\u001B[39m][idx]  \u001B[38;5;66;03m# 주입받은 경로 사용\u001B[39;00m\n\u001B[0;32m---> 50\u001B[0m     file_audio \u001B[38;5;241m=\u001B[39m \u001B[43mwfdb\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrdsamp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     51\u001B[0m     data \u001B[38;5;241m=\u001B[39m file_audio\n\u001B[1;32m     52\u001B[0m     data_new \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(data[\u001B[38;5;241m0\u001B[39m])\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/wfdb/io/record.py:2317\u001B[0m, in \u001B[0;36mrdsamp\u001B[0;34m(record_name, sampfrom, sampto, channels, pn_dir, channel_names, warn_empty, return_res)\u001B[0m\n\u001B[1;32m   2312\u001B[0m     dir_list \u001B[38;5;241m=\u001B[39m pn_dir\u001B[38;5;241m.\u001B[39msplit(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2313\u001B[0m     pn_dir \u001B[38;5;241m=\u001B[39m posixpath\u001B[38;5;241m.\u001B[39mjoin(\n\u001B[1;32m   2314\u001B[0m         dir_list[\u001B[38;5;241m0\u001B[39m], download\u001B[38;5;241m.\u001B[39mget_version(dir_list[\u001B[38;5;241m0\u001B[39m]), \u001B[38;5;241m*\u001B[39mdir_list[\u001B[38;5;241m1\u001B[39m:]\n\u001B[1;32m   2315\u001B[0m     )\n\u001B[0;32m-> 2317\u001B[0m record \u001B[38;5;241m=\u001B[39m \u001B[43mrdrecord\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2318\u001B[0m \u001B[43m    \u001B[49m\u001B[43mrecord_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2319\u001B[0m \u001B[43m    \u001B[49m\u001B[43msampfrom\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampfrom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2320\u001B[0m \u001B[43m    \u001B[49m\u001B[43msampto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2321\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2322\u001B[0m \u001B[43m    \u001B[49m\u001B[43mphysical\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2323\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpn_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpn_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2324\u001B[0m \u001B[43m    \u001B[49m\u001B[43mm2s\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   2325\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_res\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_res\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2326\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchannel_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannel_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2327\u001B[0m \u001B[43m    \u001B[49m\u001B[43mwarn_empty\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mwarn_empty\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2328\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2330\u001B[0m signals \u001B[38;5;241m=\u001B[39m record\u001B[38;5;241m.\u001B[39mp_signal\n\u001B[1;32m   2331\u001B[0m fields \u001B[38;5;241m=\u001B[39m {}\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/wfdb/io/record.py:2113\u001B[0m, in \u001B[0;36mrdrecord\u001B[0;34m(record_name, sampfrom, sampto, channels, physical, pn_dir, m2s, smooth_frames, ignore_skew, return_res, force_channels, channel_names, warn_empty)\u001B[0m\n\u001B[1;32m   2110\u001B[0m no_file \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   2111\u001B[0m sig_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 2113\u001B[0m record\u001B[38;5;241m.\u001B[39me_d_signal \u001B[38;5;241m=\u001B[39m \u001B[43m_signal\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_rd_segment\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2114\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfile_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfile_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2115\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdir_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdir_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2116\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpn_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpn_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2117\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfmt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfmt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mn_sig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mn_sig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2119\u001B[0m \u001B[43m    \u001B[49m\u001B[43msig_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msig_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbyte_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbyte_offset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2121\u001B[0m \u001B[43m    \u001B[49m\u001B[43msamps_per_frame\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msamps_per_frame\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2122\u001B[0m \u001B[43m    \u001B[49m\u001B[43mskew\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mskew\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2123\u001B[0m \u001B[43m    \u001B[49m\u001B[43minit_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrecord\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minit_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2124\u001B[0m \u001B[43m    \u001B[49m\u001B[43msampfrom\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampfrom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2125\u001B[0m \u001B[43m    \u001B[49m\u001B[43msampto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mchannels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mchannels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_skew\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_skew\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2128\u001B[0m \u001B[43m    \u001B[49m\u001B[43mno_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2129\u001B[0m \u001B[43m    \u001B[49m\u001B[43msig_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msig_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2130\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_res\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_res\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2131\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2133\u001B[0m \u001B[38;5;66;03m# Only 1 sample/frame, or frames are smoothed. Return uniform numpy array\u001B[39;00m\n\u001B[1;32m   2134\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m smooth_frames:\n\u001B[1;32m   2135\u001B[0m     \u001B[38;5;66;03m# Arrange/edit the object fields to reflect user channel\u001B[39;00m\n\u001B[1;32m   2136\u001B[0m     \u001B[38;5;66;03m# and/or signal range input\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/wfdb/io/_signal.py:1207\u001B[0m, in \u001B[0;36m_rd_segment\u001B[0;34m(file_name, dir_name, pn_dir, fmt, n_sig, sig_len, byte_offset, samps_per_frame, skew, init_value, sampfrom, sampto, channels, ignore_skew, no_file, sig_data, return_res)\u001B[0m\n\u001B[1;32m   1203\u001B[0m signals \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mlen\u001B[39m(channels)\n\u001B[1;32m   1205\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m fn \u001B[38;5;129;01min\u001B[39;00m w_file_name:\n\u001B[1;32m   1206\u001B[0m     \u001B[38;5;66;03m# Get the list of all signals contained in the dat file\u001B[39;00m\n\u001B[0;32m-> 1207\u001B[0m     datsignals \u001B[38;5;241m=\u001B[39m \u001B[43m_rd_dat_signals\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1208\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfile_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfn\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1209\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdir_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdir_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1210\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpn_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpn_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1211\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfmt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw_fmt\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1212\u001B[0m \u001B[43m        \u001B[49m\u001B[43mn_sig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mlen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdatchannel\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1213\u001B[0m \u001B[43m        \u001B[49m\u001B[43msig_len\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msig_len\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1214\u001B[0m \u001B[43m        \u001B[49m\u001B[43mbyte_offset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw_byte_offset\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1215\u001B[0m \u001B[43m        \u001B[49m\u001B[43msamps_per_frame\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw_samps_per_frame\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1216\u001B[0m \u001B[43m        \u001B[49m\u001B[43mskew\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw_skew\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1217\u001B[0m \u001B[43m        \u001B[49m\u001B[43minit_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mw_init_value\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[43m        \u001B[49m\u001B[43msampfrom\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampfrom\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1219\u001B[0m \u001B[43m        \u001B[49m\u001B[43msampto\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msampto\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1220\u001B[0m \u001B[43m        \u001B[49m\u001B[43mno_file\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mno_file\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1221\u001B[0m \u001B[43m        \u001B[49m\u001B[43msig_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msig_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1222\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1224\u001B[0m     \u001B[38;5;66;03m# Copy over the wanted signals\u001B[39;00m\n\u001B[1;32m   1225\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m cn \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(out_dat_channel[fn])):\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/wfdb/io/_signal.py:1347\u001B[0m, in \u001B[0;36m_rd_dat_signals\u001B[0;34m(file_name, dir_name, pn_dir, fmt, n_sig, sig_len, byte_offset, samps_per_frame, skew, init_value, sampfrom, sampto, no_file, sig_data)\u001B[0m\n\u001B[1;32m   1335\u001B[0m     data_to_read \u001B[38;5;241m=\u001B[39m _rd_compressed_file(\n\u001B[1;32m   1336\u001B[0m         file_name\u001B[38;5;241m=\u001B[39mfile_name,\n\u001B[1;32m   1337\u001B[0m         dir_name\u001B[38;5;241m=\u001B[39mdir_name,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1344\u001B[0m         end_frame\u001B[38;5;241m=\u001B[39msampto,\n\u001B[1;32m   1345\u001B[0m     )\n\u001B[1;32m   1346\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1347\u001B[0m     data_to_read \u001B[38;5;241m=\u001B[39m \u001B[43m_rd_dat_file\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1348\u001B[0m \u001B[43m        \u001B[49m\u001B[43mfile_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdir_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpn_dir\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfmt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstart_byte\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn_read_samples\u001B[49m\n\u001B[1;32m   1349\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1351\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m extra_flat_samples:\n\u001B[1;32m   1352\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m fmt \u001B[38;5;129;01min\u001B[39;00m UNALIGNED_FMTS:\n\u001B[1;32m   1353\u001B[0m         \u001B[38;5;66;03m# Extra number of bytes to append onto the bytes read from\u001B[39;00m\n\u001B[1;32m   1354\u001B[0m         \u001B[38;5;66;03m# the dat file.\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/wfdb/io/_signal.py:1650\u001B[0m, in \u001B[0;36m_rd_dat_file\u001B[0;34m(file_name, dir_name, pn_dir, fmt, start_byte, n_samp)\u001B[0m\n\u001B[1;32m   1648\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dir_name, file_name), \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m fp:\n\u001B[1;32m   1649\u001B[0m         fp\u001B[38;5;241m.\u001B[39mseek(start_byte)\n\u001B[0;32m-> 1650\u001B[0m         sig_data \u001B[38;5;241m=\u001B[39m \u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfromfile\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1651\u001B[0m \u001B[43m            \u001B[49m\u001B[43mfp\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mDATA_LOAD_TYPES\u001B[49m\u001B[43m[\u001B[49m\u001B[43mfmt\u001B[49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcount\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43melement_count\u001B[49m\n\u001B[1;32m   1652\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1653\u001B[0m \u001B[38;5;66;03m# Stream dat file from Physionet\u001B[39;00m\n\u001B[1;32m   1654\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1655\u001B[0m     dtype_in \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mdtype(DATA_LOAD_TYPES[fmt])\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ver4_journal.common.data.ECG_Data import ECG_Data\n",
    "from ver4_journal.common.data.create_final_data import create_final_data\n",
    "\n",
    "# 각각의 BCResNets 학습\n",
    "for dd in range(5):\n",
    "    temp = [\n",
    "        [1,0,0,0,0],\n",
    "        [0,1,0,0,0],\n",
    "        [0,0,1,0,0],\n",
    "        [0,0,0,1,0],\n",
    "        [0,0,0,0,1]\n",
    "    ]\n",
    "    \n",
    "    final_data=create_final_data(temp, dd)\n",
    "    sns.countplot(data=final_data,x='Labels')\n",
    "    ECG_train = final_data[final_data['strat_fold'].isin([1, 2, 3, 4, 5, 6, 7, 8])].reset_index(drop=True)\n",
    "    ECG_val = final_data[final_data['strat_fold'] == 9].reset_index(drop=True)\n",
    "    ECG_test = final_data[final_data['strat_fold'] == 10].reset_index(drop=True)\n",
    "\n",
    "    ECG_train = ECG_train[ECG_train['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "    ECG_val = ECG_val[ECG_val['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "    ECG_test = ECG_test[ECG_test['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "\n",
    "    # ECG_train=ECG_train.reset_index()\n",
    "    # ECG_test=ECG_test.reset_index()\n",
    "    class_0_count = (ECG_train['Labels'] == 0).sum()\n",
    "    class_1_count = (ECG_train['Labels'] == 1).sum()\n",
    "\n",
    "    print(\"Class 0 count in ECG_train:\", class_0_count)\n",
    "    print(\"Class 1 count in ECG_train:\", class_1_count)   \n",
    "\n",
    "    import torch\n",
    "    # 각 클래스의 데이터 개수 세기\n",
    "    class_counts = {\n",
    "        0: class_0_count,\n",
    "        1: class_1_count\n",
    "    }\n",
    "\n",
    "    # 클래스의 비율 계산\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_ratios = {class_label: count / total_samples for class_label, count in class_counts.items()}\n",
    "\n",
    "    # weight 계산\n",
    "    class_weights = {class_label: 1.0 / ratio for class_label, ratio in class_ratios.items()}\n",
    "\n",
    "    # weight를 tensor로 변환\n",
    "    weight_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float)\n",
    "\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "    train_dataset = ECG_Data(ECG_train)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    val_dataset=ECG_Data(ECG_val)\n",
    "    val_loader=DataLoader(val_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset=ECG_Data(ECG_test)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    first_batch, label = next(iter(train_loader))\n",
    "\n",
    "    print(f\"Batch shape: {first_batch.shape}\")\n",
    "\n",
    "    print(len(train_loader), len(val_loader), len(test_loader))\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Using {device} device\")\n",
    "    num_epochs = 50\n",
    "    tau = 2\n",
    "    #model = BCResNets(int(tau * 8), 5).to(device)\n",
    "    in_channel = 12\n",
    "    best_test_auc = float(\"-inf\")\n",
    "\n",
    "    model = BCResNets(int(tau * 8), 1, in_channel).to(device)\n",
    "    #model.load_state_dict(torch.load('/content/drive/MyDrive/bcresnet3_best_weights_5.pth'))\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'classifier_state_dict': model.classifier.state_dict()\n",
    "    }, 'model_checkpoint.pth')\n",
    "    checkpoint = torch.load('model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    num_classes = 1\n",
    "    # classifier 부분 수정\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            model.c[-2], model.c[-2], (5, 5), bias=False, groups=model.c[-2], padding=(0, 2)\n",
    "        ),\n",
    "        nn.Conv2d(model.c[-2], model.c[-1], 1, bias=False),\n",
    "        nn.BatchNorm2d(model.c[-1]),\n",
    "        nn.ReLU(True),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Conv2d(model.c[-1], num_classes, 1),\n",
    "    )\n",
    "    \"\"\"\n",
    "    #print(model)\n",
    "    model = model.to(device)\n",
    "    total_iter = len(train_loader) * num_epochs\n",
    "    iterations = 0\n",
    "    init_lr = 1e-2\n",
    "    lr_lower_limit = 0\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "    #loss_fn = torch.nn.CrossEntropyLoss(class_weights)\n",
    "    loss_fn = torch.nn.BCELoss(weight=weight_tensor[1].float().to(device))\n",
    "    #loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def acc(op, labels):\n",
    "        op = torch.softmax(op, dim=1)\n",
    "        _, preds = torch.max(op, dim=1)\n",
    "        return (preds == labels).float().mean()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_accuracy = []\n",
    "        train_loss_sum = 0\n",
    "        for data, label in train_loader:\n",
    "\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss_value = loss_fn(torch.sigmoid(output).squeeze(-1).float(), label.float())\n",
    "            loss_value.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_loss_sum += loss_value.item()\n",
    "            train_accuracy.append(acc(output, label).item())\n",
    "\n",
    "        avg_train_accuracy = np.mean(train_accuracy)\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.2f}%')\n",
    "\n",
    "        model.eval()\n",
    "        test_accuracy = []\n",
    "        test_loss_sum = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for data, label in test_loader:\n",
    "                data, label = data.to(device), label.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                loss_value = loss_fn(torch.sigmoid(output).squeeze(-1).float(), label.float())\n",
    "\n",
    "                test_loss_sum += loss_value.item()\n",
    "                test_accuracy.append(acc(output, label).item())\n",
    "\n",
    "                y_true.extend(label.cpu().numpy())\n",
    "                y_pred.extend(torch.sigmoid(output).cpu().numpy())\n",
    "\n",
    "        avg_test_accuracy = np.mean(test_accuracy)\n",
    "        avg_test_loss = test_loss_sum / len(test_loader)\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_accuracy:.2f}%')\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Calculate macro AUC\n",
    "        macro_auc_ovo = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovo')\n",
    "        macro_auc_ovr = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "        if macro_auc_ovr > best_test_auc:\n",
    "          torch.save(model.state_dict(), f'../common/best_models/{dd}model.pth')\n",
    "        print(f'Macro AUC: {macro_auc_ovo:.4f}, {macro_auc_ovr:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T00:36:46.847922Z",
     "start_time": "2024-10-09T00:36:29.366307Z"
    }
   },
   "id": "eeeb4b2a96977469",
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 테스트 데이터셋 만들기\n",
    " - 이전에는 각 분류기를 학습하는 것이기 때문에, 0,1로 레이블을 변경해주었다.\n",
    " - 하지만, 직렬로 연결하고 나서는 0,1,2,3,4로 레이블을 변경해야 한다.\n",
    "  - 훈련하는게 아니므로 train, val은 생략."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3cf76a62c849682d"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "final_data=create_final_data()\n",
    "\n",
    "ECG_test = final_data[final_data['strat_fold'] == 10].reset_index(drop=True)\n",
    "ECG_test = ECG_test[ECG_test['Labels'].isin([0,1,2,3,4])].reset_index(drop=True)\n",
    "\n",
    "test_dataset=ECG_Data(ECG_test)\n",
    "test_loader=DataLoader(test_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "ECG_test['Labels'].value_counts()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "876330316af40a42",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 현재 모델의 0~4번 순서는 데이터셋의 양에 따라 정해진게 아니다\n",
    "-  `NORM - STTC - MI - HYP - CD`로 임의로 돼있다.\n",
    "-  이를 `NORM - MI - STTC - CD - HYP`로 순서를 바꿔야 한다."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e793c230b01f3f1c"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.50\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[865   0   8   0   2  88]\n",
      " [ 40   4  16   7   2 214]\n",
      " [ 82   1 309   4   8 140]\n",
      " [ 33   2   9  22   4  47]\n",
      " [ 48   0  31   1  86  79]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.81      0.90      0.85       963\n",
      "        STTC       0.57      0.01      0.03       283\n",
      "          MI       0.83      0.57      0.67       544\n",
      "         HYP       0.65      0.19      0.29       117\n",
      "          CD       0.84      0.35      0.50       245\n",
      "\n",
      "   micro avg       0.81      0.60      0.69      2152\n",
      "   macro avg       0.74      0.40      0.47      2152\n",
      "weighted avg       0.78      0.60      0.63      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.55\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[843   0   4   0   2 114]\n",
      " [ 32   0  11   6   1 233]\n",
      " [ 64   0 293   2   8 177]\n",
      " [ 30   0   6  17   2  62]\n",
      " [ 44   0  27   1  80  93]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.83      0.88      0.85       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.86      0.54      0.66       544\n",
      "         HYP       0.65      0.15      0.24       117\n",
      "          CD       0.86      0.33      0.47       245\n",
      "\n",
      "   micro avg       0.84      0.57      0.68      2152\n",
      "   macro avg       0.64      0.38      0.45      2152\n",
      "weighted avg       0.72      0.57      0.62      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.60\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[803   0   3   0   1 156]\n",
      " [ 24   0   9   4   1 245]\n",
      " [ 45   0 266   0   9 224]\n",
      " [ 25   0   5   7   1  79]\n",
      " [ 37   0  19   1  74 114]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.86      0.83      0.85       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.88      0.49      0.63       544\n",
      "         HYP       0.58      0.06      0.11       117\n",
      "          CD       0.86      0.30      0.45       245\n",
      "\n",
      "   micro avg       0.86      0.53      0.66      2152\n",
      "   macro avg       0.64      0.34      0.41      2152\n",
      "weighted avg       0.74      0.53      0.59      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.65\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[750   0   2   0   1 210]\n",
      " [ 18   0   6   2   1 256]\n",
      " [ 33   0 241   0   7 263]\n",
      " [ 21   0   3   2   1  90]\n",
      " [ 30   0  11   1  66 137]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.88      0.78      0.83       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.92      0.44      0.60       544\n",
      "         HYP       0.40      0.02      0.03       117\n",
      "          CD       0.87      0.27      0.41       245\n",
      "\n",
      "   micro avg       0.89      0.49      0.63      2152\n",
      "   macro avg       0.61      0.30      0.37      2152\n",
      "weighted avg       0.75      0.49      0.57      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.70\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[683   0   1   0   1 278]\n",
      " [ 14   0   4   0   1 264]\n",
      " [ 26   0 209   0   4 305]\n",
      " [ 15   0   3   2   1  96]\n",
      " [ 21   0   7   0  54 163]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.90      0.71      0.79       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.93      0.38      0.54       544\n",
      "         HYP       1.00      0.02      0.03       117\n",
      "          CD       0.89      0.22      0.35       245\n",
      "\n",
      "   micro avg       0.91      0.44      0.59      2152\n",
      "   macro avg       0.74      0.27      0.34      2152\n",
      "weighted avg       0.79      0.44      0.53      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.75\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[595   0   1   0   0 367]\n",
      " [  8   0   2   0   1 272]\n",
      " [ 17   0 173   0   3 351]\n",
      " [ 11   0   3   0   0 103]\n",
      " [ 14   0   3   0  36 192]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.92      0.62      0.74       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.95      0.32      0.48       544\n",
      "         HYP       0.00      0.00      0.00       117\n",
      "          CD       0.90      0.15      0.25       245\n",
      "\n",
      "   micro avg       0.93      0.37      0.53      2152\n",
      "   macro avg       0.55      0.22      0.29      2152\n",
      "weighted avg       0.76      0.37      0.48      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.80\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[456   0   1   0   0 506]\n",
      " [  3   0   1   0   0 279]\n",
      " [ 10   0 126   0   1 407]\n",
      " [  9   0   1   0   0 107]\n",
      " [  8   0   1   0  30 206]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.94      0.47      0.63       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       0.97      0.23      0.37       544\n",
      "         HYP       0.00      0.00      0.00       117\n",
      "          CD       0.97      0.12      0.22       245\n",
      "\n",
      "   micro avg       0.95      0.28      0.44      2152\n",
      "   macro avg       0.58      0.17      0.24      2152\n",
      "weighted avg       0.78      0.28      0.40      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.85\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[253   0   0   0   0 710]\n",
      " [  2   0   0   0   0 281]\n",
      " [  4   0  77   0   0 463]\n",
      " [  5   0   0   0   0 112]\n",
      " [  2   0   0   0  15 228]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.95      0.26      0.41       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       1.00      0.14      0.25       544\n",
      "         HYP       0.00      0.00      0.00       117\n",
      "          CD       1.00      0.06      0.12       245\n",
      "\n",
      "   micro avg       0.96      0.16      0.27      2152\n",
      "   macro avg       0.59      0.09      0.16      2152\n",
      "weighted avg       0.79      0.16      0.26      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.90\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[ 36   0   0   0   0 927]\n",
      " [  0   0   0   0   0 283]\n",
      " [  0   0  34   0   0 510]\n",
      " [  1   0   0   0   0 116]\n",
      " [  0   0   0   0   9 236]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.97      0.04      0.07       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       1.00      0.06      0.12       544\n",
      "         HYP       0.00      0.00      0.00       117\n",
      "          CD       1.00      0.04      0.07       245\n",
      "\n",
      "   micro avg       0.99      0.04      0.07      2152\n",
      "   macro avg       0.59      0.03      0.05      2152\n",
      "weighted avg       0.80      0.04      0.07      2152\n",
      "\n",
      "=====================================================================\n",
      "=====================================================================\n",
      "\n",
      "Testing Serialized Model Pipeline with Threshold: 0.95\n",
      "임계값: [0.5  0.55 0.6  0.65 0.7  0.75 0.8  0.85 0.9  0.95]\n",
      "Confusion Matrix:\n",
      "[[  0   0   0   0   0 963]\n",
      " [  0   0   0   0   0 283]\n",
      " [  0   0   1   0   0 543]\n",
      " [  0   0   0   0   0 117]\n",
      " [  0   0   0   0   0 245]\n",
      " [  0   0   0   0   0   0]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NORM       0.00      0.00      0.00       963\n",
      "        STTC       0.00      0.00      0.00       283\n",
      "          MI       1.00      0.00      0.00       544\n",
      "         HYP       0.00      0.00      0.00       117\n",
      "          CD       0.00      0.00      0.00       245\n",
      "\n",
      "   micro avg       1.00      0.00      0.00      2152\n",
      "   macro avg       0.20      0.00      0.00      2152\n",
      "weighted avg       0.25      0.00      0.00      2152\n",
      "\n",
      "=====================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델 순서 정의\n",
    "sequence = [0, 2, 1, 4, 3]\n",
    "num_models = len(sequence)\n",
    "\n",
    "# 각 모델을 불러와 AUC 테스트를 수행하는 코드\n",
    "best_models = []\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 입력 채널 수 정의\n",
    "in_channel = 12\n",
    "\n",
    "# 저장된 각 모델을 로드하여 리스트에 저장\n",
    "for dd in sequence:\n",
    "    model_path = f'./best_models/{dd}model.pth'\n",
    "    model = BCResNets(int(2 * 8), 1, in_channel).to(device)  # 모델 구조 동일하게 맞춰야 함\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.eval()\n",
    "    best_models.append(model)\n",
    "\n",
    "# 임계값을 0.05씩 증가시키며 테스트\n",
    "thresholds = np.arange(0.50, 1.00, 0.05)\n",
    "\n",
    "for threshold in thresholds:\n",
    "    print(\"=====================================================================\")\n",
    "    print(f'\\nTesting Serialized Model Pipeline with Threshold: {threshold:.2f}')\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    # 테스트 데이터셋을 연결된 모델에 사용\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            # 각 데이터 포인트별로 클래스 레이블을 결정\n",
    "            for i in range(data.size(0)):  # 배치 내 각 샘플에 대해 반복\n",
    "                sample = data[i].unsqueeze(0)  # 샘플 하나를 추출\n",
    "                class_label = None\n",
    "\n",
    "                # 모든 모델을 순차적으로 거치며, 특정 클래스에 속하는지 확인\n",
    "                for idx, model in enumerate(best_models):\n",
    "                    output = model(sample)\n",
    "                    output = torch.sigmoid(output).item()  # 각 모델의 출력을 sigmoid로 변환하고 스칼라로 변환\n",
    "\n",
    "                    # 임계값을 넘는 첫 번째 클래스를 찾아 레이블로 지정\n",
    "                    if output > threshold:\n",
    "                        class_label = sequence[idx]\n",
    "                        break\n",
    "\n",
    "                # 만약 모든 모델의 출력이 임계값 이하라면, OTHER 클래스로 설정\n",
    "                if class_label is None:\n",
    "                    class_label = 100  # OTHER 클래스는(100)\n",
    "\n",
    "                # 최종 클래스 레이블 저장\n",
    "                y_true.append(label[i].item())  # 샘플의 label 저장\n",
    "                y_pred.append(class_label)  # 해당 샘플의 예측된 클래스 레이블 저장\n",
    "\n",
    "    # Confusion Matrix 계산 및 시각화\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4, 100])\n",
    "    print(f'임계값: {thresholds}')\n",
    "    print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=['NORM', 'STTC', 'MI', 'HYP', 'CD', 'OTHER'], \n",
    "                yticklabels=['NORM', 'STTC', 'MI', 'HYP', 'CD', 'OTHER'])\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(f'Confusion Matrix for Serialized Model (Threshold: {threshold:.2f})')\n",
    "    plt.show()\n",
    "    \"\"\"\n",
    "    # Classification Report 추가 (OTHER 클래스 제외)\n",
    "    print('\\nClassification Report:')\n",
    "    print(classification_report(y_true, y_pred, target_names=['NORM', 'STTC', 'MI', 'HYP', 'CD'], labels=[0, 1, 2, 3, 4], zero_division=0))\n",
    "    print(\"=====================================================================\")\n",
    "    "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-08T16:47:17.763931Z",
     "start_time": "2024-10-08T16:38:31.711359Z"
    }
   },
   "id": "39684f4bf3b7b983",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "AUC는 직렬화한 multi-stage에는 적합한 지표가 아니다.\n",
    "왜냐하면, multi-stage는 각 분류기를 학습한 것이지 하나의 모델로 학습한게 아니라\n",
    "각 stage의 확률의 합이 1이 된다는 보장이 없기 때문이다. (사용시 1이 안된다고 에러 뜸)\n",
    "따라서 이 코드를 싫행시 에러가 뜨는걸 확인할 수 있다.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 모델 순서 정의\n",
    "sequence = [0, 2, 1, 4, 3]\n",
    "num_models = len(sequence)\n",
    "\n",
    "# 각 모델을 불러와 AUC 테스트를 수행하는 코드\n",
    "best_models = []\n",
    "\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# 모델 입력 채널 수 정의\n",
    "in_channel = 12\n",
    "\n",
    "# 저장된 각 모델을 로드하여 리스트에 저장\n",
    "for dd in sequence:\n",
    "    model_path = f'./best_models/{dd}model.pth'\n",
    "    model = BCResNets(int(2 * 8), 1, in_channel).to(device)  # 모델 구조 동일하게 맞춰야 함\n",
    "    model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "    model.eval()\n",
    "    best_models.append(model)\n",
    "\n",
    "# 연결된 모델을 사용하여 입력 데이터에 대해 분류를 수행하고 AUC 테스트 수행\n",
    "print('\\nTesting Serialized Model Pipeline')\n",
    "y_true = []\n",
    "y_pred_probs = []  # 각 클래스에 대한 확률 저장\n",
    "y_pred = []  # 예측된 클래스 레이블 저장\n",
    "\n",
    "# 테스트 데이터셋을 연결된 모델에 사용\n",
    "with torch.no_grad():\n",
    "    for data, label in test_loader:\n",
    "        data, label = data.to(device), label.to(device)\n",
    "\n",
    "        # 각 데이터 포인트별로 클래스 레이블을 결정\n",
    "        for i in range(data.size(0)):  # 배치 내 각 샘플에 대해 반복\n",
    "            sample = data[i].unsqueeze(0)  # 샘플 하나를 추출\n",
    "            class_probs = torch.zeros(len(sequence))  # OTHER 클래스 제외한 각 클래스 확률 저장\n",
    "\n",
    "            # 모든 모델을 순차적으로 거치며, 특정 클래스에 속하는지 확인\n",
    "            for idx, model in enumerate(best_models):\n",
    "                output = model(sample)\n",
    "                output = torch.sigmoid(output).item()  # 각 모델의 출력을 sigmoid로 변환하고 스칼라로 변환\n",
    "                class_probs[sequence[idx]] = output\n",
    "\n",
    "            # 클래스 확률을 레이블 수로 나누어 정규화하여 AUC 계산에 사용\n",
    "            normalized_probs = class_probs / len(sequence)  # 클래스 수로 나누어 정규화\n",
    "\n",
    "            # 임계값(0.5)을 넘는 첫 번째 클래스를 찾아 레이블로 지정\n",
    "            class_label = 100  # 기본적으로 OTHER 클래스로 설정\n",
    "            for idx, prob in enumerate(class_probs):\n",
    "                if prob > 0.5:\n",
    "                    class_label = sequence[idx]\n",
    "                    break\n",
    "\n",
    "            # 최종 클래스 확률 저장 및 예측 레이블 저장\n",
    "            y_true.append(label[i].item())  # 샘플의 label 저장\n",
    "            y_pred_probs.append(normalized_probs.cpu().numpy())  # 해당 샘플의 예측된 클래스 확률 저장\n",
    "            y_pred.append(class_label)  # 해당 샘플의 예측된 클래스 레이블 저장\n",
    "\n",
    "# AUC 및 Confusion Matrix 계산\n",
    "y_true = np.array(y_true)\n",
    "y_pred_probs = np.array(y_pred_probs)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# 다중 클래스 AUC 계산 (OTHER 클래스는 AUC 계산에 사용하지 않음)\n",
    "try:\n",
    "    auc_ovo = roc_auc_score(y_true, y_pred_probs, average='macro', multi_class='ovo', labels=[0, 1, 2, 3, 4])\n",
    "    auc_ovr = roc_auc_score(y_true, y_pred_probs, average='macro', multi_class='ovr', labels=[0, 1, 2, 3, 4])\n",
    "    print(f'Macro AUC OVO for Serialized Model: {auc_ovo:.4f}')\n",
    "    print(f'Macro AUC OVR for Serialized Model: {auc_ovr:.4f}')\n",
    "    print(f'Overall Performance Metrics: OVO AUC = {auc_ovo:.4f}, OVR AUC = {auc_ovr:.4f}')\n",
    "except ValueError as e:\n",
    "    print(f\"AUC calculation error: {e}\")\n",
    "\n",
    "# Confusion Matrix 계산 및 시각화\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=[0, 1, 2, 3, 4, 100])\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['NORM', 'STTC', 'MI', 'HYP', 'CD', 'OTHER'], \n",
    "            yticklabels=['NORM', 'STTC', 'MI', 'HYP', 'CD', 'OTHER'])\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Serialized Model')\n",
    "plt.show()\n",
    "\n",
    "# Classification Report 추가 (OTHER 클래스 제외)\n",
    "print('\\nClassification Report:')\n",
    "print(classification_report(y_true, y_pred, target_names=['NORM', 'STTC', 'MI', 'HYP', 'CD'], labels=[0, 1, 2, 3, 4]))\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab0a7ab4fb247740"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
