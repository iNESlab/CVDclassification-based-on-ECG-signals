{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch.utils.data import DataLoader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T01:48:08.848672Z",
     "start_time": "2024-10-09T01:48:07.730756Z"
    }
   },
   "id": "4a2ddd3ff0184e55",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "# 현재 작업 디렉토리 가져오기\n",
    "current_dir = os.getcwd()  # 현재 Jupyter Notebook의 작업 디렉토\n",
    "env_path = os.path.join(current_dir, '../.env')\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv(env_path)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T01:48:08.863890Z",
     "start_time": "2024-10-09T01:48:08.852954Z"
    }
   },
   "id": "8bd25b100b584a0c",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "   ecg_id  patient_id   age  sex  height  weight  nurse  site     device  \\\n0       1     15709.0  56.0    1     NaN    63.0    2.0   0.0  CS-12   E   \n1       2     13243.0  19.0    0     NaN    70.0    2.0   0.0  CS-12   E   \n2       3     20372.0  37.0    1     NaN    69.0    2.0   0.0  CS-12   E   \n3       4     17014.0  24.0    0     NaN    82.0    2.0   0.0  CS-12   E   \n4       5     17448.0  19.0    1     NaN    70.0    2.0   0.0  CS-12   E   \n\n        recording_date  ... validated_by_human  baseline_drift static_noise  \\\n0  1984-11-09 09:17:34  ...               True             NaN    , I-V1,     \n1  1984-11-14 12:55:37  ...               True             NaN          NaN   \n2  1984-11-15 12:49:10  ...               True             NaN          NaN   \n3  1984-11-15 13:44:57  ...               True    , II,III,AVF          NaN   \n4  1984-11-17 10:43:15  ...               True   , III,AVR,AVF          NaN   \n\n  burst_noise electrodes_problems  extra_beats  pacemaker  strat_fold  \\\n0         NaN                 NaN          NaN        NaN           3   \n1         NaN                 NaN          NaN        NaN           2   \n2         NaN                 NaN          NaN        NaN           5   \n3         NaN                 NaN          NaN        NaN           3   \n4         NaN                 NaN          NaN        NaN           4   \n\n                 filename_lr                filename_hr  \n0  records100/00000/00001_lr  records500/00000/00001_hr  \n1  records100/00000/00002_lr  records500/00000/00002_hr  \n2  records100/00000/00003_lr  records500/00000/00003_hr  \n3  records100/00000/00004_lr  records500/00000/00004_hr  \n4  records100/00000/00005_lr  records500/00000/00005_hr  \n\n[5 rows x 28 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ecg_id</th>\n      <th>patient_id</th>\n      <th>age</th>\n      <th>sex</th>\n      <th>height</th>\n      <th>weight</th>\n      <th>nurse</th>\n      <th>site</th>\n      <th>device</th>\n      <th>recording_date</th>\n      <th>...</th>\n      <th>validated_by_human</th>\n      <th>baseline_drift</th>\n      <th>static_noise</th>\n      <th>burst_noise</th>\n      <th>electrodes_problems</th>\n      <th>extra_beats</th>\n      <th>pacemaker</th>\n      <th>strat_fold</th>\n      <th>filename_lr</th>\n      <th>filename_hr</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15709.0</td>\n      <td>56.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>63.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-09 09:17:34</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>, I-V1,</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>records100/00000/00001_lr</td>\n      <td>records500/00000/00001_hr</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>13243.0</td>\n      <td>19.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-14 12:55:37</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>records100/00000/00002_lr</td>\n      <td>records500/00000/00002_hr</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>20372.0</td>\n      <td>37.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>69.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-15 12:49:10</td>\n      <td>...</td>\n      <td>True</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>5</td>\n      <td>records100/00000/00003_lr</td>\n      <td>records500/00000/00003_hr</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>17014.0</td>\n      <td>24.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>82.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-15 13:44:57</td>\n      <td>...</td>\n      <td>True</td>\n      <td>, II,III,AVF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>3</td>\n      <td>records100/00000/00004_lr</td>\n      <td>records500/00000/00004_hr</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>17448.0</td>\n      <td>19.0</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>70.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>CS-12   E</td>\n      <td>1984-11-17 10:43:15</td>\n      <td>...</td>\n      <td>True</td>\n      <td>, III,AVR,AVF</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>4</td>\n      <td>records100/00000/00005_lr</td>\n      <td>records500/00000/00005_hr</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 28 columns</p>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db_path=os.getenv(\"DATA_PATH\")\n",
    "ecg_data=pd.read_csv(f'{db_path}/ptbxl_database.csv')\n",
    "ecg_data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T01:48:08.981848Z",
     "start_time": "2024-10-09T01:48:08.866052Z"
    }
   },
   "id": "41d7712d02c12fdc",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 모델 및 하이퍼파라미터 준비\n",
    "BCResNet의 cnn_first의 stride나 다른 conv의 stride에 따라 입력 또는 출력의 크기가 달라짐.\n",
    "이는 조절해가면서 실험필요"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92dc6ab07ba70006"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T01:48:09.781344Z",
     "start_time": "2024-10-09T01:48:09.678994Z"
    }
   },
   "id": "a0c7f0b5f5c4c757",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 12, 1000])\n"
     ]
    },
    {
     "data": {
      "text/plain": "torch.Size([1, 5])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from ver4_journal.common.network.BCResNets import BCResNets\n",
    "\n",
    "tau = 1\n",
    "in_channel = 12\n",
    "model = BCResNets(int(tau * 8), 5, in_channel)\n",
    "\n",
    "if in_channel == 1:\n",
    "    sample = torch.randn(1,12,1000)\n",
    "else:\n",
    "    sample = torch.randn(1,12,1000)\n",
    "    #sample = torch.randn(1,12,20,48)\n",
    "\n",
    "print(sample.shape)\n",
    "pred = model(sample)\n",
    "pred.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T01:48:12.217189Z",
     "start_time": "2024-10-09T01:48:12.184576Z"
    }
   },
   "id": "8060e95182a8bfb7",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0 count in ECG_train: 9415\n",
      "Class 1 count in ECG_train: 7596\n",
      "Class weights: {0: np.float64(1.8067976633032394), 1: np.float64(2.239468141126909)}\n",
      "Weight tensor: tensor([1.8068, 2.2395])\n",
      "Batch shape: torch.Size([32, 12, 1000])\n",
      "532 67 68\n",
      "Using mps device\n",
      "Epoch: 1/50, Training Loss: 1.0424, Training Accuracy: 0.55%\n",
      "Test Loss: 1.0614, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9047, 0.9047\n",
      "Epoch: 2/50, Training Loss: 0.8353, Training Accuracy: 0.55%\n",
      "Test Loss: 0.9085, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9132, 0.9132\n",
      "Epoch: 3/50, Training Loss: 0.7861, Training Accuracy: 0.55%\n",
      "Test Loss: 0.9383, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9126, 0.9126\n",
      "Epoch: 4/50, Training Loss: 0.7706, Training Accuracy: 0.55%\n",
      "Test Loss: 0.9065, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9177, 0.9177\n",
      "Epoch: 5/50, Training Loss: 0.7630, Training Accuracy: 0.55%\n",
      "Test Loss: 0.9000, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9164, 0.9164\n",
      "Epoch: 6/50, Training Loss: 0.7462, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8406, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9188, 0.9188\n",
      "Epoch: 7/50, Training Loss: 0.7310, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8528, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9254, 0.9254\n",
      "Epoch: 8/50, Training Loss: 0.7142, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8416, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9266, 0.9266\n",
      "Epoch: 9/50, Training Loss: 0.7132, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7945, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9313, 0.9313\n",
      "Epoch: 10/50, Training Loss: 0.7075, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8592, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9242, 0.9242\n",
      "Epoch: 11/50, Training Loss: 0.7142, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8957, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9264, 0.9264\n",
      "Epoch: 12/50, Training Loss: 0.6939, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7774, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9321, 0.9321\n",
      "Epoch: 13/50, Training Loss: 0.6921, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8138, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9266, 0.9266\n",
      "Epoch: 14/50, Training Loss: 0.6847, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8262, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9316, 0.9316\n",
      "Epoch: 15/50, Training Loss: 0.6826, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7979, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9317, 0.9317\n",
      "Epoch: 16/50, Training Loss: 0.6659, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8016, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9323, 0.9323\n",
      "Epoch: 17/50, Training Loss: 0.6640, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7778, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9333, 0.9333\n",
      "Epoch: 18/50, Training Loss: 0.6675, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7986, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9291, 0.9291\n",
      "Epoch: 19/50, Training Loss: 0.6586, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8392, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9339, 0.9339\n",
      "Epoch: 20/50, Training Loss: 0.6515, Training Accuracy: 0.55%\n",
      "Test Loss: 1.6479, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.7235, 0.7235\n",
      "Epoch: 21/50, Training Loss: 0.6579, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7889, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9330, 0.9330\n",
      "Epoch: 22/50, Training Loss: 0.6475, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7678, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9361, 0.9361\n",
      "Epoch: 23/50, Training Loss: 0.6415, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7678, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9353, 0.9353\n",
      "Epoch: 24/50, Training Loss: 0.6459, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7994, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9348, 0.9348\n",
      "Epoch: 25/50, Training Loss: 0.6349, Training Accuracy: 0.55%\n",
      "Test Loss: 0.8080, Test Accuracy: 0.56%\n",
      "Macro AUC: 0.9373, 0.9373\n",
      "Epoch: 26/50, Training Loss: 0.6357, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7469, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9386, 0.9386\n",
      "Epoch: 27/50, Training Loss: 0.6338, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7578, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9353, 0.9353\n",
      "Epoch: 28/50, Training Loss: 0.6315, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7885, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9307, 0.9307\n",
      "Epoch: 29/50, Training Loss: 0.6183, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7813, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9373, 0.9373\n",
      "Epoch: 30/50, Training Loss: 0.6225, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7675, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9377, 0.9377\n",
      "Epoch: 31/50, Training Loss: 0.6123, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7552, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9368, 0.9368\n",
      "Epoch: 32/50, Training Loss: 0.6176, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7915, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9341, 0.9341\n",
      "Epoch: 33/50, Training Loss: 0.6099, Training Accuracy: 0.55%\n",
      "Test Loss: 0.7667, Test Accuracy: 0.55%\n",
      "Macro AUC: 0.9379, 0.9379\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 126\u001B[0m\n\u001B[1;32m    123\u001B[0m data, label \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mto(device), label\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m    125\u001B[0m optim\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m--> 126\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    127\u001B[0m loss_value \u001B[38;5;241m=\u001B[39m loss_fn(torch\u001B[38;5;241m.\u001B[39msigmoid(output)\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mfloat(), label\u001B[38;5;241m.\u001B[39mfloat())\n\u001B[1;32m    128\u001B[0m loss_value\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/ver4_journal/common/network/BCResNets.py:73\u001B[0m, in \u001B[0;36mBCResNets.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     71\u001B[0m         x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mBCBlocks[i][j](x)\n\u001B[1;32m     72\u001B[0m         \u001B[38;5;66;03m#print(x.shape)\u001B[39;00m\n\u001B[0;32m---> 73\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclassifier\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     74\u001B[0m x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, x\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[1;32m     75\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/container.py:219\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    217\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 219\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    220\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1551\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1552\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1557\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1558\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1560\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1561\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1562\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1564\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1565\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/modules/batchnorm.py:176\u001B[0m, in \u001B[0;36m_BatchNorm.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    169\u001B[0m     bn_training \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_mean \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrunning_var \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[1;32m    171\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001B[39;00m\n\u001B[1;32m    173\u001B[0m \u001B[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001B[39;00m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001B[39;00m\n\u001B[1;32m    175\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m--> 176\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    177\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    178\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001B[39;49;00m\n\u001B[1;32m    179\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_mean\u001B[49m\n\u001B[1;32m    180\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\n\u001B[1;32m    181\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    182\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrunning_var\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrack_running_stats\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    183\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    184\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    185\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbn_training\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    186\u001B[0m \u001B[43m    \u001B[49m\u001B[43mexponential_average_factor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    187\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    188\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/CVDclassification-based-on-ECG-signals/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2512\u001B[0m, in \u001B[0;36mbatch_norm\u001B[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001B[0m\n\u001B[1;32m   2509\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m training:\n\u001B[1;32m   2510\u001B[0m     _verify_batch_size(\u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m-> 2512\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbatch_norm\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2513\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_mean\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrunning_var\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtraining\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmomentum\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackends\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcudnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43menabled\u001B[49m\n\u001B[1;32m   2514\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkgAAAGwCAYAAABSN5pGAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAsLUlEQVR4nO3dfXhU5YH38d8kw0wGMQJJiMRQpNQiQhiGhBdX6CVoqVT2keWtxZbIQoVLiaxrURuiEIKRZwNUFwNiqkBQViGAuKCLLttqa7WhDSYRIW6QqllIdBKNETOZIck8f2jP49y8bIgJM8Hv57pymTn3OWfuo5z4Zc6ZiS0YDAYFAAAAS1S4JwAAABBpCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAY7OGeQFdWV/e5+BxyAAC6BptNiou7tE3rEkjfQDAoAgkAgIsQl9gAAAAMBBIAAICBQAIAADAQSAAAAIaICKRAIKDJkyeruLjYWlZaWqqf/vSn8ng8+tGPfqSioqKQbd544w1NnjxZbrdb6enpqqqqChnfvHmzxo0bJ4/HoyVLlsjn81ljfr9fS5YsUVpamsaOHauNGzd27gECAIAuJeyB5Pf7dc8996iystJa5vV6dfvtt2vUqFF6/vnntWjRIq1YsUKvvvqqJOnEiRNauHChpk6dqh07dqh379668847FfzqLWUvv/yy8vPzlZOTo8LCQpWVlWnVqlXW/vPy8nTo0CEVFhZq2bJlys/P1759+y7ocQMAgMgV1kA6evSoZs6cqQ8//DBk+f79+xUfH6977rlHV155pW6++WZNmTJFe/bskSQVFRVp6NChmjt3rq666iqtXLlSx48f14EDByRJW7Zs0W233abx48dr2LBhWr58uXbu3Cmfz6fGxkYVFRUpKytLQ4YM0Q9/+EP94he/0NatWy/48QMAgMgU1s9BOnDggEaPHq1//ud/1vDhw63l48aN0+DBg09b/+TJk5KksrIypaWlWctdLpeGDBmi0tJSpaWl6e2331ZGRoY1Pnz4cJ06dUoVFRUKBoNqbm6Wx+OxxlNTU7Vhwwa1trYqKqrtzWiznc/RAgCAcDqf/2+HNZBuvfXWMy5PTk5WcnKy9biurk4vvvii7rrrLklfXoLr06dPyDZxcXGqqalRQ0OD/H5/yLjdblfPnj1VU1OjqKgo9erVSw6HwxqPj4+X3+9XfX29evfu3eb5t/XTOAEAQNcS8Z+k3dTUpLvuukvx8fH6yU9+Ikny+XwhgSNJDodDgUBATU1N1uMzjQeDwTOOSV/eLH4++FUjAAB0HRfNrxr54osvdOedd+r999/Xv/3bv8nlckmSnE7naTETCAQUGxsrp9NpPTbHXS6XWlpazjgmSTExMec1P37VCAAAF6ewv4vtbE6ePKl58+apsrJShYWFuvLKK62xxMRE1dbWhqxfW1urhIQE9ezZU06nM2S8ublZ9fX1SkhIUGJioj799FM1Nzdb416vVzExMYqNje304wIAAJEvIgOptbVVGRkZ+p//+R89/fTTuuqqq0LG3W63SkpKrMc+n0+HDx+W2+1WVFSUUlJSQsZLS0tlt9t19dVXa/DgwbLb7SotLbXGS0pKlJKScl43aAMAgItXRBbBjh07VFxcrIceekixsbHyer3yer2qr6+XJE2bNk0HDx5UQUGBKisrlZmZqeTkZI0ePVrSlzd/P/XUU9q/f7/Ky8uVnZ2tmTNnyuVyyeVyacqUKcrOzlZ5ebn279+vjRs3Kj09PYxHDAAAIoktGIyMu2gGDRqkLVu2aPTo0Zo3b55ef/3109YZNWqUnn76aUnSa6+9pocfflg1NTXyeDxasWKF+vXrZ61bUFCgzZs3KxAIaOLEiVq2bJl1f5LP51N2drZeeeUV9ejRQ/PmzdOcOXPOe861tZ17k7bdHqXo6IhsWCBsWlpa1dzcGu5pAOiCbDYpPr5tN2lHTCB1RZ0ZSHZ7lHpeFiNbVHTnPAHQRQVbW1T/WRORBOC8nU8gRfS72L7NoqOjZIuKVu2uX+lU7bFwTweICN3iv6v4qf9X0dFRBBKATkUgRbhTtcd0quZIuKcBAMC3Cje4AAAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAAAAMBBIAAIAhIgIpEAho8uTJKi4utpZVVVVpzpw5Gj58uH784x/r9ddfD9nmjTfe0OTJk+V2u5Wenq6qqqqQ8c2bN2vcuHHyeDxasmSJfD6fNeb3+7VkyRKlpaVp7Nix2rhxY+ceIAAA6FLCHkh+v1/33HOPKisrrWXBYFALFy5UfHy8du7cqVtuuUUZGRk6ceKEJOnEiRNauHChpk6dqh07dqh379668847FQwGJUkvv/yy8vPzlZOTo8LCQpWVlWnVqlXW/vPy8nTo0CEVFhZq2bJlys/P1759+y7sgQMAgIgV1kA6evSoZs6cqQ8//DBk+Z/+9CdVVVUpJydHAwcO1IIFCzR8+HDt3LlTklRUVKShQ4dq7ty5uuqqq7Ry5UodP35cBw4ckCRt2bJFt912m8aPH69hw4Zp+fLl2rlzp3w+nxobG1VUVKSsrCwNGTJEP/zhD/WLX/xCW7duveDHDwAAIlNYA+nAgQMaPXq0tm3bFrK8rKxM11xzjbp3724tS01NVWlpqTWelpZmjblcLg0ZMkSlpaVqaWnR22+/HTI+fPhwnTp1ShUVFaqoqFBzc7M8Hk/IvsvKytTa2tpJRwoAALoSezif/NZbbz3jcq/Xqz59+oQsi4uLU01Nzf863tDQIL/fHzJut9vVs2dP1dTUKCoqSr169ZLD4bDG4+Pj5ff7VV9fr969e7d5/jZbm1cF0ME4/wCcr/P5uRHWQDobn88XEjCS5HA4FAgE/tfxpqYm6/GZxoPB4BnHJFn7b6u4uEvPa30AHSM21hXuKQC4yEVkIDmdTtXX14csCwQCiomJscbNmAkEAoqNjZXT6bQem+Mul0stLS1nHJNk7b+t6uo+11f3hXc4h8PO/wSAs2ho8CkQaA73NAB0MTZb21/ciMhASkxM1NGjR0OW1dbWWpfNEhMTVVtbe9r44MGD1bNnTzmdTtXW1mrgwIGSpObmZtXX1yshIUHBYFCffvqpmpubZbd/efher1cxMTGKjY09r3kGg+q0QAJwbpx7ADpT2N/mfyZut1vvvPOOdblMkkpKSuR2u63xkpISa8zn8+nw4cNyu92KiopSSkpKyHhpaansdruuvvpqDR48WHa73brh+2/7TklJUVRURP7rAAAAF1hEFsGoUaPUt29fZWZmqrKyUgUFBSovL9f06dMlSdOmTdPBgwdVUFCgyspKZWZmKjk5WaNHj5b05c3fTz31lPbv36/y8nJlZ2dr5syZcrlccrlcmjJlirKzs1VeXq79+/dr48aNSk9PD+chAwCACBKRl9iio6O1fv16ZWVlaerUqerfv7/WrVunpKQkSVJycrIee+wxPfzww1q3bp08Ho/WrVsn21e3p9988806fvy4li5dqkAgoIkTJ+ree++19p+Zmans7Gzddttt6tGjh+666y5NnDgxLMcKAAAijy0Y5Ep+e9XWdt5N2k7nlzdpVxfM1KmaI53zJEAX0+3yweo7f7saGnzy+7lJG8D5sdmk+Pi23aQdkZfYAAAAwolAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAABDRH4OEgBc7Oz2KEVH83dU4OtaWlrV3Nwa7mlIIpAA4IKz26PU87IY2aKiwz0VIKIEW1tU/1lTREQSgQQAF1h0dJRsUdGq3fUrnao9Fu7pABGhW/x3FT/1/yo6OopAAoBvs1O1x/ikfCBCcQEcAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgCGiA6m6uloLFizQiBEjNGHCBG3evNkaO3z4sGbMmCG3261p06bp0KFDIdvu3btXN954o9xutxYuXKhPPvnEGgsGg1q9erXGjBmjUaNGKS8vT62trRfqsAAAQISL6EC6++671b17d+3atUtLlizRo48+qv/8z/9UY2Oj5s+fr7S0NO3atUsej0cLFixQY2OjJKm8vFxZWVnKyMjQtm3b1NDQoMzMTGu/mzZt0t69e5Wfn6+1a9dqz5492rRpU7gOEwAARJiIDaTPPvtMpaWluuOOO3TllVfqxhtv1Lhx4/Tmm2/qpZdektPp1H333aeBAwcqKytLl1xyifbt2ydJeuaZZzRp0iRNmTJFV199tfLy8vTaa6+pqqpKkrRlyxYtWrRIaWlpGjNmjBYvXqytW7eG83ABAEAEidhAiomJkcvl0q5du3Tq1CkdO3ZMBw8e1ODBg1VWVqbU1FTZbDZJks1m04gRI1RaWipJKisrU1pamrWvvn37KikpSWVlZfroo49UXV2tkSNHWuOpqak6fvy4Pv744wt6jAAAIDJFbCA5nU4tXbpU27Ztk9vt1qRJk/SDH/xAM2bMkNfrVZ8+fULWj4uLU01NjSTp448/Puu41+uVpJDx+Ph4SbK2byubrfO+AJxbZ55/nf0F4Nwi4fyzd97hfXPvvfeexo8fr3/8x39UZWWlVqxYoWuvvVY+n08OhyNkXYfDoUAgIElqamo663hTU5P1+Otjkqzt2you7tLzPiYA31xsrCvcUwDQSSLl/I7YQHrzzTe1Y8cOvfbaa4qJiVFKSoo++ugjPf744+rXr99pMRMIBBQTEyPpy1efzjTucrlCYsjpdFrfS5LLdX7/UerqPlcw2K7D+185HPaI+UMCRJqGBp8CgeZwT6PdOL+Bs+vM89tma/uLGxF7ie3QoUPq37+/FT2SdM011+jEiRNKTExUbW1tyPq1tbXWZbOzjSckJCgxMVGSrEttX/8+ISHhvOYYDHbeF4Bz68zzr7O/AJxbJJx/ERtIffr00QcffBDyStCxY8eUnJwst9utt956S8GvjjQYDOrgwYNyu92SJLfbrZKSEmu76upqVVdXy+12KzExUUlJSSHjJSUlSkpKOu2+JQAA8O0UsYE0YcIEdevWTQ888ID++te/6re//a02bNig2bNn66abblJDQ4Nyc3N19OhR5ebmyufzadKkSZKkWbNm6YUXXlBRUZEqKip033336frrr1e/fv2s8dWrV6u4uFjFxcVas2aN0tPTw3m4AAAggkTsPUiXXnqpNm/erNzcXE2fPl29e/fWHXfcoZ/85Cey2Wx64okntGzZMm3fvl2DBg1SQUGBunfvLknyeDzKycnR2rVr9dlnn+m6667TihUrrH3PmzdPdXV1ysjIUHR0tKZPn645c+aE6UgBAECksQWDXBFvr9razrtJ2+n88ibO6oKZOlVzpHOeBOhiul0+WH3nb1dDg09+f9e9SZvzGzjdhTi/bTYpPr6L36QNAAAQLgQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADO0KpPT0dDU0NJy2/JNPPtHUqVO/8aQAAADCyd7WFX//+9+rvLxckvTnP/9ZGzZsUPfu3UPW+eCDD3T8+PGOnSEAAMAF1uZAGjBggJ588kkFg0EFg0EdPHhQ3bp1s8ZtNpu6d++u3NzcTpkoAADAhdLmQOrXr5+2bNkiScrMzFRWVpZ69OjRaRMDAAAIlzYH0tetXLlSkuT1etXc3KxgMBgynpSU9M1nBgAAECbtCqQ//vGPevDBB1VdXS1JCgaDstls1j+PHDnSoZMEAAC4kNoVSDk5ORo2bJgef/xxLrMBAICLTrsCqaamRk8++aT69evX0fMBAAAIu3Z9DlJaWppKSko6ei4AAAARoV2vII0cOVLLly/Xq6++qv79+4e83V+SMjIyOmRyAAAA4dDum7SHDh2quro61dXVhYzZbLYOmRgAAEC4tCuQnn766Y6eBwAAQMRoVyDt3r37nONTpkxpz24BAAAiQrsCae3atSGPW1paVFdXJ7vdrmHDhhFIAACgS2tXIP32t789bdkXX3yhpUuXatCgQd94UgAAAOHUrrf5n8kll1yiu+66S5s2beqoXSoQCGj58uUaOXKk/u7v/k6//vWvrV9rcvjwYc2YMUNut1vTpk3ToUOHQrbdu3evbrzxRrndbi1cuFCffPKJNRYMBrV69WqNGTNGo0aNUl5enlpbWzts3gAAoGvrsECSpIqKig4NjYceekhvvPGGnnrqKa1Zs0bbt2/Xtm3b1NjYqPnz5ystLU27du2Sx+PRggUL1NjYKEkqLy9XVlaWMjIytG3bNjU0NCgzM9Pa76ZNm7R3717l5+dr7dq12rNnT4eGHQAA6NradYlt9uzZp72d/4svvtC7776rOXPmdMS8VF9fr507d2rTpk0aNmyYJGnu3LkqKyuT3W6X0+nUfffdJ5vNpqysLP3+97/Xvn37NHXqVD3zzDOaNGmSdS9UXl6exo8fr6qqKvXr109btmzRokWLlJaWJklavHix/vVf/1Xz5s3rkLkDAICurV2BNHr06NOWORwOLV68WNdee+03npQklZSUqEePHho1apS1bP78+ZKkBx98UKmpqVak2Ww2jRgxQqWlpZo6darKysp0++23W9v17dtXSUlJKisrk8PhUHV1tUaOHGmNp6am6vjx4/r444/Vp0+fNs+Rj3wCwofzD7h4ddb5fT77bVcgff2Tsk+ePKmWlhZddtll7dnVWVVVVemKK67Q7t27tWHDBp06dUpTp07VHXfcIa/Xq+9973sh68fFxamyslKSzhg6cXFxqqmpkdfrlaSQ8fj4eElf/o658wmkuLhL23VsAL6Z2FhXuKcAoJNEyvndrkCSpMLCQj355JOqra2VJPXu3VuzZs3qsF8z0tjYqA8++EDPPfecVq5cKa/Xq6VLl8rlcsnn88nhcISs73A4FAgEJElNTU1nHW9qarIef31MkrV9W9XVfa6v7hnvcA6HPWL+kACRpqHBp0CgOdzTaDfOb+DsOvP8ttna/uJGuwJp3bp1euaZZ/RP//RP8ng8am1t1cGDB5Wfny+Hw2FdCvsm7Ha7Tp48qTVr1uiKK66QJJ04cULPPvus+vfvf1rMBAIBxcTESJKcTucZx10uV0gMOZ1O63tJcrnO7wdWMKhOCyQA58a5B1y8IuH8blcgbd++Xbm5uZowYYK1bPDgwUpMTFRubm6HBFJCQoKcTqcVR5I0YMAAVVdXa9SoUdYrV39TW1trXR5LTEw843hCQoISExMlSV6vV8nJydb3f3tOAACAdr3N/+TJk7ryyitPWz5gwICQzxv6Jtxut/x+v/76179ay44dO6YrrrhCbrdbb731lvWZSMFgUAcPHpTb7ba2LSkpsbarrq5WdXW13G63EhMTlZSUFDJeUlKipKSk87r/CAAAXLzaFUgej0cbN24M+cyjlpYWPfXUU9Zb8r+p7373u7r++uuVmZmpiooK/eEPf1BBQYFmzZqlm266SQ0NDcrNzdXRo0eVm5srn8+nSZMmSZJmzZqlF154QUVFRaqoqNB9992n66+/Xv369bPGV69ereLiYhUXF2vNmjVKT0/vkHkDAICur12X2DIzM/Wzn/1Mb7zxhoYMGSJJeueddxQIBPTkk0922ORWr16tFStWaNasWXK5XPrZz35mfQbTE088oWXLlmn79u0aNGiQCgoK1L17d0lfBlxOTo7Wrl2rzz77TNddd51WrFhh7XfevHmqq6tTRkaGoqOjNX369A77/CYAAND12YLB9t0K9e///u+qr6/XsWPH5HQ6tXXrVq1duzbkvqSLXW1t572Lzen88l0u1QUzdarmSOc8CdDFdLt8sPrO366GBp/8/q77LjbOb+B0F+L8ttmk+Pi2vYutXZfYnn76aWVnZ+vSSy9Vdna2MjMzNXv2bC1evFjbt29vzy4BAAAiRrsCadOmTVqzZo3+4R/+wVp2//33a9WqVSooKOiwyQEAAIRDuwLp008/1Xe+853Tlg8YMOC0t9cDAAB0Ne0KpNTUVD322GPy+XzWMr/frw0bNsjj8XTY5AAAAMKhXe9iW7p0qebOnauxY8dan4f04YcfKj4+XuvXr+/I+QEAAFxw7Qqk73znO3rppZf0hz/8Qe+//77sdruuvPJKjR07VtHR0R09RwAAgAuq3b+s1uFw6IYbbujIuQAAAESEdt2DBAAAcDEjkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGAgkAAAAAwEEgAAgIFAAgAAMBBIAAAABgIJAADAQCABAAAYCCQAAAADgQQAAGDoMoE0f/58/epXv7IeHz58WDNmzJDb7da0adN06NChkPX37t2rG2+8UW63WwsXLtQnn3xijQWDQa1evVpjxozRqFGjlJeXp9bW1gt2LAAAILJ1iUB68cUX9dprr1mPGxsbNX/+fKWlpWnXrl3yeDxasGCBGhsbJUnl5eXKyspSRkaGtm3bpoaGBmVmZlrbb9q0SXv37lV+fr7Wrl2rPXv2aNOmTRf8uAAAQGSK+ECqr69XXl6eUlJSrGUvvfSSnE6n7rvvPg0cOFBZWVm65JJLtG/fPknSM888o0mTJmnKlCm6+uqrlZeXp9dee01VVVWSpC1btmjRokVKS0vTmDFjtHjxYm3dujUsxwcAACJPxAfSv/zLv+iWW27R9773PWtZWVmZUlNTZbPZJEk2m00jRoxQaWmpNZ6Wlmat37dvXyUlJamsrEwfffSRqqurNXLkSGs8NTVVx48f18cff3xec7PZOu8LwLl15vnX2V8Azi0Szj975x3eN/fmm2/qL3/5i/bs2aPs7GxrudfrDQkmSYqLi1NlZaUk6eOPP1afPn1OG6+pqZHX65WkkPH4+HhJUk1NzWnbnUtc3KXndTwAOkZsrCvcUwDQSSLl/I7YQPL7/Vq2bJmWLl2qmJiYkDGfzyeHwxGyzOFwKBAISJKamprOOt7U1GQ9/vqYJGv7tqqr+1zB4Hlt0mYOhz1i/pAAkaahwadAoDnc02g3zm/g7Drz/LbZ2v7iRsQGUn5+voYOHapx48adNuZ0Ok+LmUAgYIXU2cZdLldIDDmdTut7SXK5zu8HVjCoTgskAOfGuQdcvCLh/I7YQHrxxRdVW1srj8cj6f9HzMsvv6zJkyertrY2ZP3a2lrr8lhiYuIZxxMSEpSYmCjpy8t0ycnJ1veSlJCQ0HkHBAAAuoyIvUn76aef1p49e7R7927t3r1bEyZM0IQJE7R792653W699dZbCn6VmMFgUAcPHpTb7ZYkud1ulZSUWPuqrq5WdXW13G63EhMTlZSUFDJeUlKipKSk87r/CAAAXLwi9hWkK664IuTxJZdcIknq37+/4uLitGbNGuXm5uqnP/2pnnvuOfl8Pk2aNEmSNGvWLM2ePVvDhw9XSkqKcnNzdf3116tfv37W+OrVq3X55ZdLktasWaO5c+dewKMDAACRLGID6Vx69OihJ554QsuWLdP27ds1aNAgFRQUqHv37pIkj8ejnJwcrV27Vp999pmuu+46rVixwtp+3rx5qqurU0ZGhqKjozV9+nTNmTMnTEcDAAAijS0YjIRbobqm2trOexeb0/nlu1yqC2bqVM2RznkSoIvpdvlg9Z2/XQ0NPvn9XfddbJzfwOkuxPlts0nx8W17F1vE3oMEAAAQLgQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAENEB9JHH32kRYsWadSoURo3bpxWrlwpv98vSaqqqtKcOXM0fPhw/fjHP9brr78esu0bb7yhyZMny+12Kz09XVVVVSHjmzdv1rhx4+TxeLRkyRL5fL4LdlwAACCyRWwgBYNBLVq0SD6fT1u3btUjjzyi3/3ud3r00UcVDAa1cOFCxcfHa+fOnbrllluUkZGhEydOSJJOnDihhQsXaurUqdqxY4d69+6tO++8U8FgUJL08ssvKz8/Xzk5OSosLFRZWZlWrVoVzsMFAAARJGID6dixYyotLdXKlSt11VVXKS0tTYsWLdLevXv1pz/9SVVVVcrJydHAgQO1YMECDR8+XDt37pQkFRUVaejQoZo7d66uuuoqrVy5UsePH9eBAwckSVu2bNFtt92m8ePHa9iwYVq+fLl27tzJq0gAAEBSBAdSQkKCnnzyScXHx4csP3nypMrKynTNNdeoe/fu1vLU1FSVlpZKksrKypSWlmaNuVwuDRkyRKWlpWppadHbb78dMj58+HCdOnVKFRUVnXtQAACgS7CHewJnExsbq3HjxlmPW1tb9cwzz2jMmDHyer3q06dPyPpxcXGqqamRpHOONzQ0yO/3h4zb7Xb17NnT2r6tbLbzPSoAHYXzD7h4ddb5fT77jdhAMq1atUqHDx/Wjh07tHnzZjkcjpBxh8OhQCAgSfL5fGcdb2pqsh6fbfu2iou79HwPA0AHiI11hXsKADpJpJzfXSKQVq1apcLCQj3yyCP6/ve/L6fTqfr6+pB1AoGAYmJiJElOp/O02AkEAoqNjZXT6bQem+Mu1/n9R6mr+1xf3ffd4RwOe8T8IQEiTUODT4FAc7in0W6c38DZdeb5bbO1/cWNiL0H6W9WrFihTZs2adWqVfrRj34kSUpMTFRtbW3IerW1tdZls7ONJyQkqGfPnnI6nSHjzc3Nqq+vV0JCwnnNLRjsvC8A59aZ519nfwE4t0g4/yI6kPLz8/Xcc8/p17/+tW6++WZrudvt1jvvvGNdLpOkkpISud1ua7ykpMQa8/l8Onz4sNxut6KiopSSkhIyXlpaKrvdrquvvvoCHBUAAIh0ERtI7733ntavX6/bb79dqamp8nq91teoUaPUt29fZWZmqrKyUgUFBSovL9f06dMlSdOmTdPBgwdVUFCgyspKZWZmKjk5WaNHj5Yk3XrrrXrqqae0f/9+lZeXKzs7WzNnzjzvS2wAAODiFLH3IP3Xf/2XWlpa9Pjjj+vxxx8PGXv33Xe1fv16ZWVlaerUqerfv7/WrVunpKQkSVJycrIee+wxPfzww1q3bp08Ho/WrVsn21e3r9988806fvy4li5dqkAgoIkTJ+ree++94McIAAAiky0Y5Ip4e9XWdt5N2k7nlzdxVhfM1KmaI53zJEAX0+3yweo7f7saGnzy+7vuTdqc38DpLsT5bbNJ8fEXyU3aAAAAFxqBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABi+tYHk9/u1ZMkSpaWlaezYsdq4cWO4pwQAACKEPdwTCJe8vDwdOnRIhYWFOnHihO6//34lJSXppptuCvfUAABAmH0rA6mxsVFFRUX6zW9+oyFDhmjIkCGqrKzU1q1bCSQAAPDtvMRWUVGh5uZmeTwea1lqaqrKysrU2toaxpkBAIBI8K18Bcnr9apXr15yOBzWsvj4ePn9ftXX16t3795t2k9UlBQMdtYsv+S4fLBs3Vyd+yRAF9Et7krr+6iL4K93nN/A/3chzm+bre3rfisDyefzhcSRJOtxIBBo83569760Q+d1JnH/Z3mnPwfQ1cTGXhxRwfkNnC5Szu+L4O9g58/pdJ4WQn97HBMTE44pAQCACPKtDKTExER9+umnam5utpZ5vV7FxMQoNjY2jDMDAACR4FsZSIMHD5bdbldpaam1rKSkRCkpKYq6GG5sAAAA38i3sgZcLpemTJmi7OxslZeXa//+/dq4caPS09PDPTUAABABbMFgZ78PKzL5fD5lZ2frlVdeUY8ePTRv3jzNmTMn3NMCAAAR4FsbSAAAAGfzrbzEBgAAcC4EEgAAgIFAAgAAMBBIwDn4/X4tWbJEaWlpGjt2rDZu3BjuKQHoYIFAQJMnT1ZxcXG4p4II8q38VSNAW+Xl5enQoUMqLCzUiRMndP/99yspKUk33XRTuKcGoAP4/X798pe/VGVlZbingghDIAFn0djYqKKiIv3mN7/RkCFDNGTIEFVWVmrr1q0EEnAROHr0qH75y1+KN3PjTLjEBpxFRUWFmpub5fF4rGWpqakqKytTa2trGGcGoCMcOHBAo0eP1rZt28I9FUQgXkECzsLr9apXr15yOBzWsvj4ePn9ftXX16t3795hnB2Ab+rWW28N9xQQwXgFCTgLn88XEkeSrMeBQCAcUwIAXCAEEnAWTqfztBD62+OYmJhwTAkAcIEQSMBZJCYm6tNPP1Vzc7O1zOv1KiYmRrGxsWGcGQCgsxFIwFkMHjxYdrtdpaWl1rKSkhKlpKQoKopTBwAuZvyUB87C5XJpypQpys7OVnl5ufbv36+NGzcqPT093FMDAHQy3sUGnENmZqays7N12223qUePHrrrrrs0ceLEcE8LANDJbEE+IQsAACAEl9gAAAAMBBIAAICBQAIAADAQSAAAAAYCCQAAwEAgAQAAGAgkAAAAA4EEAABgIJAAXBQGDRqk4uLidm07e/ZsPfbYY+3atri4WIMGDWrXtgAiF4EEAABgIJAAAAAMBBKAi14wGNSGDRs0YcIEDR06VGPHjlV+fn7IOjU1Nfr5z3+ulJQUzZw5UxUVFdZYQ0OD7r33Xo0YMUJjx47VihUr1NTUdMbn2rJli8aPH6+UlBRNnTpVf/nLXzr12AB0DgIJwEVv9+7dKiwsVG5urvbt26eFCxfqscce0zvvvGOt8/zzz+umm27S7t271a9fP2VkZKilpUWSlJWVpc8//1zPPvus1q9fr7fffls5OTmnPc/hw4eVl5enZcuW6T/+4z+Ulpamu+++W62trRfsWAF0DAIJwEWvb9++Wrlypa699lolJydr1qxZSkhIUGVlpbXOjTfeqJ///OcaOHCgli9frrq6Ov3xj3/Uhx9+qP3792vVqlUaNGiQhg0bphUrVuj555/X559/HvI8x48fl81mU1JSkpKTk3X33Xdr1apVBBLQBdnDPQEA6GxjxoxRWVmZ1qxZo/fee09HjhyR1+sNCZdhw4ZZ3/fo0UMDBgzQsWPH1NLSotbWVv3gBz8I2Wdra6s++OCDkGVjx47V97//ff393/+9rrnmGt1www2aMWOG7HZ+1AJdDWctgIteUVGRHn74Yc2YMUMTJ07U/fffr/T09JB1oqOjQx63traqW7duamlp0aWXXqqdO3eett/ExESVlZVZj10ul4qKinTgwAH97ne/065du/Tss89q165dSkxM7JyDA9ApuMQG4KL37LPPauHChVqyZImmTJmiXr16qa6uTsFg0Frnv//7v63vGxoa9P777+u73/2uBgwYoM8//1w2m039+/dX//791dTUpLy8PAUCgZDneeutt/TEE09ozJgxyszM1L59++T3+1VSUnLBjhVAx+AVJAAXjfLycvn9/pBlI0eOVK9evfTmm2/qhhtu0BdffKFHHnlEp06dCgmcPXv2yOPxaMSIEXr00UfVv39/jRkzRjabTePGjdPixYv1wAMPKDo6Wg8++KAuu+wyxcbGhjxXTEyM1q1bp/j4eF177bX685//rMbGRj5IEuiCbMGv/xUKALqos0XIK6+8oubmZi1ZskRHjhxRXFycJk2apA8//FC9e/dWTk6OZs+eraFDh6qkpERHjhyRx+NRbm6u+vXrJ0n65JNP9NBDD+nVV1+V3W7XuHHj9MADD6hXr14qLi5Wenq63n33XUnSCy+8oPXr1+vEiRNKSkrSokWLdPPNN1+wfw8AOgaBBAAAYOAeJAAAAAOBBAAAYCCQAAAADAQSAACAgUACAAAwEEgAAAAGAgkAAMBAIAEAABgIJAAAAAOBBAAAYCCQAAAADP8PqLC6+caYq68AAAAASUVORK5CYII="
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ver4_journal.common.data.ECG_Data import ECG_Data\n",
    "from ver4_journal.common.data.create_final_data import create_final_data\n",
    "\n",
    "# 각각의 BCResNets 학습\n",
    "for dd in range(5):\n",
    "    temp = [\n",
    "        [1,0,0,0,0],\n",
    "        [0,1,0,0,0],\n",
    "        [0,0,1,0,0],\n",
    "        [0,0,0,1,0],\n",
    "        [0,0,0,0,1]\n",
    "    ]\n",
    "    \n",
    "    final_data=create_final_data(temp, dd)\n",
    "    sns.countplot(data=final_data,x='Labels')\n",
    "    ECG_train = final_data[final_data['strat_fold'].isin([1, 2, 3, 4, 5, 6, 7, 8])].reset_index(drop=True)\n",
    "    ECG_val = final_data[final_data['strat_fold'] == 9].reset_index(drop=True)\n",
    "    ECG_test = final_data[final_data['strat_fold'] == 10].reset_index(drop=True)\n",
    "\n",
    "    ECG_train = ECG_train[ECG_train['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "    ECG_val = ECG_val[ECG_val['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "    ECG_test = ECG_test[ECG_test['Labels'].isin([0,1])].reset_index(drop=True)\n",
    "\n",
    "    # ECG_train=ECG_train.reset_index()\n",
    "    # ECG_test=ECG_test.reset_index()\n",
    "    class_0_count = (ECG_train['Labels'] == 0).sum()\n",
    "    class_1_count = (ECG_train['Labels'] == 1).sum()\n",
    "\n",
    "    print(\"Class 0 count in ECG_train:\", class_0_count)\n",
    "    print(\"Class 1 count in ECG_train:\", class_1_count)   \n",
    "\n",
    "    import torch\n",
    "    # 각 클래스의 데이터 개수 세기\n",
    "    class_counts = {\n",
    "        0: class_0_count,\n",
    "        1: class_1_count\n",
    "    }\n",
    "\n",
    "    # 클래스의 비율 계산\n",
    "    total_samples = sum(class_counts.values())\n",
    "    class_ratios = {class_label: count / total_samples for class_label, count in class_counts.items()}\n",
    "\n",
    "    # weight 계산\n",
    "    class_weights = {class_label: 1.0 / ratio for class_label, ratio in class_ratios.items()}\n",
    "\n",
    "    # weight를 tensor로 변환\n",
    "    weight_tensor = torch.tensor(list(class_weights.values()), dtype=torch.float)\n",
    "\n",
    "    print(\"Class weights:\", class_weights)\n",
    "    print(\"Weight tensor:\", weight_tensor)\n",
    "\n",
    "    train_dataset = ECG_Data(ECG_train)\n",
    "    train_loader = DataLoader(train_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    val_dataset=ECG_Data(ECG_val)\n",
    "    val_loader=DataLoader(val_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    test_dataset=ECG_Data(ECG_test)\n",
    "    test_loader=DataLoader(test_dataset,batch_size=32, shuffle=True)\n",
    "\n",
    "    first_batch, label = next(iter(train_loader))\n",
    "\n",
    "    print(f\"Batch shape: {first_batch.shape}\")\n",
    "\n",
    "    print(len(train_loader), len(val_loader), len(test_loader))\n",
    "\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"Using {device} device\")\n",
    "    num_epochs = 50\n",
    "    tau = 2\n",
    "    #model = BCResNets(int(tau * 8), 5).to(device)\n",
    "    in_channel = 12\n",
    "    best_test_auc = float(\"-inf\")\n",
    "\n",
    "    model = BCResNets(int(tau * 8), 1, in_channel).to(device)\n",
    "    #model.load_state_dict(torch.load('/content/drive/MyDrive/bcresnet3_best_weights_5.pth'))\n",
    "\n",
    "    \"\"\"\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'classifier_state_dict': model.classifier.state_dict()\n",
    "    }, 'model_checkpoint.pth')\n",
    "    checkpoint = torch.load('model_checkpoint.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "    num_classes = 1\n",
    "    # classifier 부분 수정\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Conv2d(\n",
    "            model.c[-2], model.c[-2], (5, 5), bias=False, groups=model.c[-2], padding=(0, 2)\n",
    "        ),\n",
    "        nn.Conv2d(model.c[-2], model.c[-1], 1, bias=False),\n",
    "        nn.BatchNorm2d(model.c[-1]),\n",
    "        nn.ReLU(True),\n",
    "        nn.AdaptiveAvgPool2d((1, 1)),\n",
    "        nn.Conv2d(model.c[-1], num_classes, 1),\n",
    "    )\n",
    "    \"\"\"\n",
    "    #print(model)\n",
    "    model = model.to(device)\n",
    "    total_iter = len(train_loader) * num_epochs\n",
    "    iterations = 0\n",
    "    init_lr = 1e-2\n",
    "    lr_lower_limit = 0\n",
    "\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=init_lr)\n",
    "    #loss_fn = torch.nn.CrossEntropyLoss(class_weights)\n",
    "    loss_fn = torch.nn.BCELoss(weight=weight_tensor[1].float().to(device))\n",
    "    #loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def acc(op, labels):\n",
    "        op = torch.softmax(op, dim=1)\n",
    "        _, preds = torch.max(op, dim=1)\n",
    "        return (preds == labels).float().mean()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_accuracy = []\n",
    "        train_loss_sum = 0\n",
    "        for data, label in train_loader:\n",
    "\n",
    "            data, label = data.to(device), label.to(device)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            output = model(data)\n",
    "            loss_value = loss_fn(torch.sigmoid(output).squeeze(-1).float(), label.float())\n",
    "            loss_value.backward()\n",
    "            optim.step()\n",
    "\n",
    "            train_loss_sum += loss_value.item()\n",
    "            train_accuracy.append(acc(output, label).item())\n",
    "\n",
    "        avg_train_accuracy = np.mean(train_accuracy)\n",
    "        avg_train_loss = train_loss_sum / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1}/{num_epochs}, Training Loss: {avg_train_loss:.4f}, Training Accuracy: {avg_train_accuracy:.2f}%')\n",
    "\n",
    "        model.eval()\n",
    "        test_accuracy = []\n",
    "        test_loss_sum = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "        with torch.no_grad():\n",
    "            for data, label in test_loader:\n",
    "                data, label = data.to(device), label.to(device)\n",
    "\n",
    "                output = model(data)\n",
    "                loss_value = loss_fn(torch.sigmoid(output).squeeze(-1).float(), label.float())\n",
    "\n",
    "                test_loss_sum += loss_value.item()\n",
    "                test_accuracy.append(acc(output, label).item())\n",
    "\n",
    "                y_true.extend(label.cpu().numpy())\n",
    "                y_pred.extend(torch.sigmoid(output).cpu().numpy())\n",
    "\n",
    "        avg_test_accuracy = np.mean(test_accuracy)\n",
    "        avg_test_loss = test_loss_sum / len(test_loader)\n",
    "        print(f'Test Loss: {avg_test_loss:.4f}, Test Accuracy: {avg_test_accuracy:.2f}%')\n",
    "\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "\n",
    "        # Calculate macro AUC\n",
    "        macro_auc_ovo = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovo')\n",
    "        macro_auc_ovr = roc_auc_score(y_true, y_pred, average='macro', multi_class='ovr')\n",
    "        if macro_auc_ovr > best_test_auc:\n",
    "          torch.save(model.state_dict(), f'../common/best_models/{dd}model.pth')\n",
    "        print(f'Macro AUC: {macro_auc_ovo:.4f}, {macro_auc_ovr:.4f}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-09T02:51:35.712104Z",
     "start_time": "2024-10-09T01:50:32.130378Z"
    }
   },
   "id": "eeeb4b2a96977469",
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
